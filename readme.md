# Replicate TinyStories to TinyCoder a sub-100MB coding agent.

Hypothesis: This project itself would be an experiment to check if we could build LLM small enough to do one task perfectly i.e coding within sub-100MB.

and lastly this is a self-learning project, once trained lots of post-training methods would be applied, for e.g knowledge distillation, RLHF, Lora, Quanitization.....



- [ ] Get dataset from tinystories
- [ ] replicate tiny stories model by following the paper.
- [ ] experiment LSTM, RNN layers to reduce size.
- [ ] if done try to do same with coding dataset.
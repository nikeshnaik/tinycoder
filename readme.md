# Replicate TinyStories to TinyCoder a sub-100MB coding agent.

Hypothesis: This project itself would be an experiment to check if we could build LLM small enough to do one task perfectly i.e coding within sub-100MB.

and lastly this is a self-learning project, once trained lots of post-training methods would be applied, for e.g knowledge distillation, RLHF, Lora, Quanitization.....



- [X] Get dataset from tinystories
- [X] replicate tiny stories model config by following the paper.
- [X] dry test for 10 epochs
- [X] add wanddb to log
- [ ] start training for 8hours
- [ ] experiment LSTM, RNN layers to reduce size.
- [ ] if done try to do same with coding dataset.
